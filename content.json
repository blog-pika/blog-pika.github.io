{"meta":{"title":"The Code and Sixpence","subtitle":null,"description":null,"author":"Developer Pikachu","url":"https://blog-pika.github.io","root":"/"},"pages":[{"title":"categories","date":"2019-08-07T03:05:45.000Z","updated":"2019-08-07T03:05:56.727Z","comments":true,"path":"categories/index.html","permalink":"https://blog-pika.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-08-07T02:54:50.000Z","updated":"2019-08-07T03:04:12.703Z","comments":true,"path":"tags/index.html","permalink":"https://blog-pika.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"DDIA - Ch8 - Pitfalls in Distributed System","slug":"DDIA/DDIA-Ch8-Pitfalls-in-distribuetd-system","date":"2019-08-11T07:00:00.000Z","updated":"2019-08-12T05:59:15.199Z","comments":true,"path":"2019/08/11/DDIA/DDIA-Ch8-Pitfalls-in-distribuetd-system/","link":"","permalink":"https://blog-pika.github.io/2019/08/11/DDIA/DDIA-Ch8-Pitfalls-in-distribuetd-system/","excerpt":"","text":"AbstractThis blog is my reading notes for Designing Data-Intensive Applications, Chapter 08: The Trouble With Distributed System. Working with distributed systems is fundamentally different from writing software on a single computer. The main difference is that we can know nothing for sure. This chapter first introduces how components can be unreliable in a distributed system, and why designing distributed system can be challenging. Then the author discusses how to get truth from uncertainty. He also introduces some theoretical models and how can a distributed algorithm defined as correct. Typical UnReliable ComponentUnreliable NetworkMost distributed systems are asynchronous networks and have unbounded delays, that is, they try to deliver packets as quickly as possible, but there is no upper limit on the time it may take for a packet to arrive. So Whenever you try to send a packet over the network, it may be lost or arbitrarily delayed. Likewise, the reply may be lost or delayed, so if you don’t get a reply, you have no idea whether the message got through. Though TCP retries transparently, in the application level, you may also retry a few times, wait for a timeout to elapse, and eventually declare the node dead if you don’t hear back within the timeout. Even better, rather than using configured constant timeouts, systems can continually measure response times and their variability, and automatically adjust timeouts according to the observed response time distribution. Akka and Cassandra use a Phi Accrual failure detector to dynamically adjust the timeout. You do need to know how your software reacts to network problems and ensure that the system can recover from them. If your network is normally fairly reliable, a valid approach may be to simply show an error message to users while your network is experiencing problems. Unreliable ClockEach machine on the network has its own clock, which is an actual hardware device: usually a quartz crystal oscillator. The quartz clock in a computer is not very accurate: it drifts (runs faster or slower than it should), so each machine has its own notion of time, which may be slightly faster or slower than on other machines. A commonly used mechanism to synchronize clocks is the Network Time Protocol (NTP), which allows the computer clock to be adjusted according to the time reported by a group of servers. Modern computers have at least two different kinds of clocks: a time-of-day clock to get timestamp, and a monotonic clock to get time duration. Time-of-day clock returns the current date and time according to some calendar. However, it may be forcibly reset by the NTP server, suddenly jump forward or back in time. Even worse, NTP synchronization can only be as good as the network delay, so there is a limit to its accuracy when you’re on a congested network with variable packet delays. Therefore, replying on Time-of-day clock is dangerous. On the contrary, NTP cannot cause the monotonic clock to jump forward or backward. Since a monotonic clock is guaranteed to always move forward, it is usually fine to use a monotonic clock to measure time duration in distributed system. There are two ways to get reliable timestamps Logical Clocks: which are based on incrementing counters rather than an oscillating quartz crystal, are a safer alternative for ordering events. Logical clocks do not measure the time of day or the number of seconds elapsed, only the relative ordering of events. Google Spanner provides a TrueTime API, which returns [earliest, latest]. The interval are the earliest possible and the latest possible timestamp. Therefore, the clock knows that the actual current time is somewhere within that interval. Since Spanner uses atomic clocks, the interval are usually within 7ms. In order to ensure that transaction timestamps reflect causality, Spanner deliberately waits for the length of the confidence interval before committing a read-write transaction. By doing so, it ensures that any transaction that may read the data is at a sufficiently later time, so their confidence intervals do not overlap. A problem of clock is that even if it is defective, most things will seem to work fine. If some piece of software is relying on an accurately synchronized clock, the result is more likely to be silent and subtle data loss than a dramatic crash. Thus, if you use software that requires synchronized clocks, it is essential that you also carefully monitor the clock offsets between all the machines. Any node whose clock drifts too far from the others should be declared dead and removed from the cluster. Process PauseA running thread can be preempted at any point and resume it at some later time, without the thread even noticing: e.g.: Many programming language runtime have a GC that occasionally needs to stop all running threads, which can pauses for several minutes in the worst case! Even so-called “concurrent” garbage collectors cannot fully run in parallel with the application code Execution may also be suspended and resumed arbitrarily, e.g., when VM migrates from one host to another host, when the operating system context-switches to another thread, when the user closes the lid of their laptop If the application performs synchronous disk access, a thread may be paused waiting for a slow disk I/O operation to complete. If the operating system is configured to allow paging, a simple memory access may result in a page fault that requires a page from disk to be loaded into memory. In many languages, disk access can happen surprisingly, even if the code doesn’t explicitly mention file access. For example, the Java class loader lazily loads class files when they are first used, which could happen at any time in the program execution. A Unix process can be paused by sending it the SIGSTOP signal Therefore, we can’t assume anything about timing. A node in a distributed system must assume that its execution can be paused for a significant length of time at any point, even in the middle of a function. During the pause, the rest of the world keeps moving and may even declare the paused node dead because it’s not responding. Eventually, the paused node may continue running, without even noticing that it was asleep until it checks its clock sometime later. In the code below, if code enters if (lease.isValid() but suddenly preempted. During its pause other nodes can become the new leader. If it continues to process request as it is supposed to do as a leader, the system may go wrong. 123456789101112while (true) &#123; request = getIncomingRequest(); // Ensure that the lease always has at least 10 seconds remaining if (lease.expiryTimeMillis - System.currentTimeMillis() &lt; 10000) &#123; lease = lease.renew(); &#125; if (lease.isValid()) &#123; process(request); &#125;&#125; We can limit the impact of garbage collection in the following two ways: Treat GC pauses like brief planned outages of a node, and to let other nodes handle requests from clients while one node is collecting its garbage. If the runtime can warn the application that a node soon requires a GC pause, the application can stop sending new requests to that node, wait for it to finish processing outstanding requests, and then perform the GC while no requests are in progress Use the garbage collector only for short-lived objects and to restart processes periodically, before they accumulate enough long-lived objects to require a full GC of long-lived objects Challenges in Distributed SystemUncertainty is the defining characteristic of distributed systems. Whenever software tries to do anything involving other nodes, there is the possibility that it may occasionally fail, or randomly go slow, or not respond at all (and eventually time out). In distributed systems, we try to build tolerance of partial failures into software, so that the system as a whole may continue functioning even when some of its constituent parts are broken. To tolerate faults, the first step is to detect them, but even that is hard. Most systems don’t have an accurate mechanism of detecting whether a node has failed, so most distributed algorithms rely on timeouts to determine whether a remote node is still available. However, timeouts can’t distinguish between network and node failures, and variable network delay sometimes causes a node to be falsely suspected of crashing. Once a fault is detected, making a system tolerate it is not easy either: there is no global variable, no shared memory, no common knowledge or any other kind of shared state between the machines. Nodes can’t even agree on what time it is, let alone on anything more profound. The only way information can flow from one node to another is by sending it over the unreliable network. Major decisions cannot be safely made by a single node, so we require protocols that enlist help from other nodes and try to get a quorum to agree. If you’re used to writing software in the idealized mathematical perfection of a single computer, where the same operation always deterministically returns the same result, then moving to the messy physical reality of distributed systems can be a bit of a shock. Conversely, distributed systems engineers will often regard a problem as trivial if it can be solved on a single computer, and indeed a single computer can do a lot nowadays. If you can avoid opening Pandora’s box and simply keep things on a single machine, it is generally worth doing so. However, scalability is not the only reason for wanting to use a distributed system. Fault tolerance and low latency (by placing data geographically close to users) are equally important goals, and those things cannot be achieved with a single node. How to know the TruthA node cannot necessarily trust its own judgment of a situation. A distributed system cannot exclusively rely on a single node, because a node may fail at any time, potentially leaving the system stuck and unable to recover. Instead, many distributed algorithms rely on a quorum, that is, voting among the nodes: decisions require some minimum number of votes from several nodes in order to reduce the dependence on any one particular node. Most commonly, the quorum is an absolute majority of more than half the nodes. System Model and Algorithms ScopeAlgorithms for distributed system need to be written in a way that does not depend too heavily on the details of the hardware and software configuration on which they are run. This in turn requires that we somehow formalize the kinds of faults that we expect to happen in a system. We do this by defining a system model, which is an abstraction that describes what things an algorithm may assume. With regard to timing assumptions, three system models are in common use: Synchronous model: The synchronous model assumes bounded network delay, bounded process pauses, and bounded clock error. Bounded does not imply exactly zero; it just means that they will never exceed some fixed upper bound. The synchronous model is not a realistic model of most practical systems Partially synchronous model: Partial synchrony means that a system behaves like a synchronous system most of the time, but it sometimes exceeds the bounds for network delay, process pauses, and clock drift. This is a realistic model of many systems. Asynchronous model: In this model, an algorithm is not allowed to make any timing assumptions. In fact, it does not even have a clock, so it cannot use timeouts. Some algorithms can be designed for the asynchronous model, but it is very restrictive. With regard to node failures. The three most common system models are: Crash-stop faults: a node can fail in only by crashing: suddenly stop responding at any moment, and thereafter that node is gone foreverCrash-recovery faults: nodes may crash at any moment, and perhaps start responding again after some unknown time. Stable storage is assumed to be preserved across crashes, while the in-memory state is assumed to be lostByzantine faults: Nodes may do absolutely anything, including trying to trick and deceive other nodes. For modeling real systems, the partially synchronous model with crash-recovery faults is generally the most useful model. But how do distributed algorithms cope with that model? To define what it means for an algorithm to be correct, we need to define safety and liveness properties. Safety is often informally defined as nothing bad happens, and liveness as something good eventually happens.: Safety: If a safety property is violated, we can point at a particular point in time at which it was broken (for example, if the uniqueness property was violated, we can identify the particular operation in which a duplicate fencing token was returned). After a safety property has been violated, the violation cannot be undone—the damage is already done. Liveness: may not hold at some point in time (for example, a node may have sent a request but not yet received a response), but there is always hope that it may be satisfied in the future (namely by receiving a response). For distributed algorithms, it is common to require that safety properties always hold, in all possible situations of a system model. That is, even if all nodes crash, or the entire network fails, the algorithm must nevertheless ensure that it does not return a wrong result. However, with liveness properties we are allowed to make caveats: for example, we could say that a request needs to receive a response only if a majority of nodes have not crashed, and only if the network eventually recovers from an outage. The definition of the partially synchronous model requires that eventually the system returns to a synchronous state—that is, any period of network interruption lasts only for a finite duration and is then repaired. Reference [1] Designing Data-Intensive Applications, Chapter 08: The Trouble With Distributed System","categories":[{"name":"Reading Notes","slug":"Reading-Notes","permalink":"https://blog-pika.github.io/categories/Reading-Notes/"},{"name":"Designing Data-Intensive Applications","slug":"Reading-Notes/Designing-Data-Intensive-Applications","permalink":"https://blog-pika.github.io/categories/Reading-Notes/Designing-Data-Intensive-Applications/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"https://blog-pika.github.io/tags/Distributed-System/"},{"name":"DDIA","slug":"DDIA","permalink":"https://blog-pika.github.io/tags/DDIA/"}]},{"title":"DDIA - Ch6 - Partition","slug":"DDIA/DDIA-Ch6-Partition","date":"2019-08-05T07:00:00.000Z","updated":"2019-08-09T03:01:42.849Z","comments":true,"path":"2019/08/05/DDIA/DDIA-Ch6-Partition/","link":"","permalink":"https://blog-pika.github.io/2019/08/05/DDIA/DDIA-Ch6-Partition/","excerpt":"AbstractThis blog is my reading notes for Designing Data-Intensive Applications, Chapter 06: Partition. Partitioning means splitting a large dataset into smaller subsets, which are then placed on different nodes in a shared-nothing cluster. It is necessary when you have so much data that storing and processing it on a single machine is no longer feasible. Also it has advantages over scalability: a large dataset can be distributed across many disks, and the query load can be distributed across many processors.","text":"AbstractThis blog is my reading notes for Designing Data-Intensive Applications, Chapter 06: Partition. Partitioning means splitting a large dataset into smaller subsets, which are then placed on different nodes in a shared-nothing cluster. It is necessary when you have so much data that storing and processing it on a single machine is no longer feasible. Also it has advantages over scalability: a large dataset can be distributed across many disks, and the query load can be distributed across many processors. In this blog we will look at the following topics one by one: How to partition datasets and avoid hot spot? How to rebalance Data? How to use secondary indexes in partition? How to route request? Approaches to PartitioningWe have following ways of partition: Key Range Partition: assign a continuous range of keys to each partition. Within each partition, keys are kept in sorted order. The advantage is that efficient range queries are possible, but there is a risk of hot spots if the application often accesses keys that are close together in the sorted order. Partitions are typically rebalanced dynamically by splitting the range into two sub-ranges when a partition gets too big. Hash Partition: a hash function is applied to each key, and a partition owns a range of hashes. Range queries are inefficient, but data are distributed more evenly. When partitioning by hash, it is common to create a fixed number of partitions in advance, to assign several partitions to each node, and to move partitions from one node to another when nodes are added or removed. Dynamic partitioning can also be used. Hybrid Partition: a table can be declared with a compound primary key consisting of several columns. Only the first part of that key is hashed to determine the partition, but the other columns are used as a concatenated index for sorting the data. This approach enables an elegant data model for one-to-many relationships. For example, on a social media site, one user may post many updates. If the primary key for updates is chosen to be (user_id, update_timestamp), then you can efficiently retrieve all updates made by a particular user within some time interval, sorted by timestamp. If the partitioning is unfair, so that some partitions have more data or queries than others, we call it skewed. A partition with disproportionately high load is called a hot spot. Today, most data systems are not able to automatically compensate for such a highly skewed workload, so it’s the responsibility of the application to reduce the skew. One simple technique is that, if one key is known to be very hot, add a random number to the beginning or end of the key can split the writes to the key evenly across different keys, i.e., distributed to different partitions. The disadvantage is that this technique now requires additional bookkeeping and reading the data now need to query from all keys and combine them. RebalancingRebalancing is the process of moving load from one node in the cluster to another, which is called for the following changes: query throughput increases, dataset size increases, node fails. Usually it is expected to meet some minimum requirements: After rebalancing, the load should be shared fairly between the nodes in the cluster. While rebalancing is happening, the database should continue accepting reads and writes. No more data than necessary should be moved between nodes, to make rebalancing fast and to minimize the network and disk I/O load. We also have 3 approaches to do that: Fixed number of partitions: can be used if the dataset is not highly variable: create many more partitions than there are nodes, and assign several partitions to each node. If a node is added to the cluster, the new node can steal a few partitions from every existing node until partitions are fairly distributed once again. In this partition, the number of partitions is usually fixed when the database is first set up and not changed afterward. Also, partition splitting is usually not implemented. The number of partitions configured at the outset = the maximum number of nodes * number of partitions per node, so you need to choose it high enough to accommodate future growth. Used in Riak, Elasticsearch, Couchbase, and Voldemort Dynamic partitioning proportional to the size of the dataset: When a partition grows to exceed a configured size, it is split into two partitions so that approximately half of the data ends up on each side of the split. Conversely, if lots of data is deleted and a partition shrinks below some threshold, it can be merged with an adjacent partition. In the initial state when the dataset is small or empty, an initial set of partitions to be configured. Used by HBase and MongoDB Dynamic partitioning proportional to the number of nodes: When a new node joins the cluster, it randomly chooses a fixed number of existing partitions to split, and then takes ownership of one half of each of those split partitions while leaving the other half of each partition in place. The randomization can produce unfair splits, but when averaged over a larger number of partitions, the new node ends up taking a fair share of the load from the existing nodes. Used in Cassandra and Ketama. Fully automated rebalancing can be convenient but unpredictable. Rebalancing is an expensive operation, if it is not done carefully, this process can overload the network or the nodes and harm the performance of other requests while the rebalancing is in progress. In combination with automatic failure detection, other nodes can conclude an overloaded node to be dead and automatically rebalance the cluster to move load away from it, which may cause a cascading failure. Secondary indexsince dataset is partitioned by primary key, secondary indexes don’t map neatly to partitions. Thus, many key-value stores have avoided secondary indexes because of their added implementation complexity, but some have started adding them because they are so useful for data modeling. There are two methods: Document-partitioned indexes: each partition is completely separate and maintains its own local secondary indexes, covering only the documents in that partition. This means that only a single partition needs to be updated on write, but a read of the secondary index requires a scatter/gather across all partitions, which is expensive. Term-partitioned indexes: construct a global secondary index that covers data in all partitions. The global index is also partitioned to avoid bottleneck, but it can be partitioned differently from the primary key index. When a document is written, a distributed transaction across all partitions that need to be updated is required, which is expensive. However, a read can be served from a single partition and is efficient. Request RoutingWhen a client wants to make a request, how does it know which node to connect to? On a high level, there are a few different approaches to this problem. Allow clients to contact any node. If that node does not own the partition, it forwards the request to the appropriate node Send all requests from clients to a routing tier first, which determines the node that should handle each request and forwards it accordingly. This routing tier does not itself handle any requests; it only acts as a partition-aware load balancer. Require that clients be aware of the partitioning and the assignment of partitions to nodes, thus they can connect directly to the appropriate node without any intermediary. In all cases, the key problem is: how does the component making the routing decision and learn about changes in the assignment of partitions to nodes? This is an instance of a more general problem called service discovery Any piece of software that is accessible over a network has this problem, especially if it is aiming for high availability. Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata. Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. Other actors, such as the routing tier or the partitioning-aware client, can subscribe to this information in ZooKeeper. Whenever a partition changes ownership, or a node is added or removed, ZooKeeper notifies the routing tier so that it can keep its routing information up to date. Or the system can use a gossip protocol among the nodes to disseminate any changes in cluster state. Requests can be sent to any node, and that node forwards them to the appropriate node for the requested partition. Cassandra and Riak adopt this way. Reference [1] Designing Data-Intensive Applications, Chapter 06: Partition","categories":[{"name":"Reading Notes","slug":"Reading-Notes","permalink":"https://blog-pika.github.io/categories/Reading-Notes/"},{"name":"Designing Data-Intensive Applications","slug":"Reading-Notes/Designing-Data-Intensive-Applications","permalink":"https://blog-pika.github.io/categories/Reading-Notes/Designing-Data-Intensive-Applications/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"https://blog-pika.github.io/tags/Distributed-System/"},{"name":"DDIA","slug":"DDIA","permalink":"https://blog-pika.github.io/tags/DDIA/"},{"name":"Partition","slug":"Partition","permalink":"https://blog-pika.github.io/tags/Partition/"}]},{"title":"DDIA - Ch8 - Replication","slug":"DDIA/DDIA-Ch5-Replicattion","date":"2019-08-01T07:00:00.000Z","updated":"2019-08-09T03:02:50.013Z","comments":true,"path":"2019/08/01/DDIA/DDIA-Ch5-Replicattion/","link":"","permalink":"https://blog-pika.github.io/2019/08/01/DDIA/DDIA-Ch5-Replicattion/","excerpt":"AbstractThis blog is my reading notes for Designing Data-Intensive Applications, Chapter 05: Replication. Replication means keeping a copy of the same data on multiple machines that are connected via a network. It has following benefits: High availability: Keeping the system running, even when one/several machines goes down Latency: Placing data geographically close to users, so that users can interact with it faster Scalability: Being able to handle a higher volume of reads than a single machine could handle, by performing reads on replicas","text":"AbstractThis blog is my reading notes for Designing Data-Intensive Applications, Chapter 05: Replication. Replication means keeping a copy of the same data on multiple machines that are connected via a network. It has following benefits: High availability: Keeping the system running, even when one/several machines goes down Latency: Placing data geographically close to users, so that users can interact with it faster Scalability: Being able to handle a higher volume of reads than a single machine could handle, by performing reads on replicas However, keeping data consistent between replicas are not as simple as it looks. We need to consider the following issues: node failures; unreliable networks; and tradeoffs around replica consistency, durability, availability, and latency. Let’s look at them one by one: should we use synchronous or asynchronous? which granularity of Replication Logs should we use? which kind of replication architecture: Single-leader, multiple-leader, or leaderless? should we provide consistency stronger than eventual consistency? How to handle node failure and unreliable network? How to handle concurrency write conflicts? Synchronous vs Asynchronous synchronous: the leader waits until follower has confirmed that it received the write before reporting success to the user, and before making the write visible to other clients. asynchronous: the leader sends the message, but doesn’t wait for a response from the follower. semi-synchronous: one of the followers is synchronous, and the others are asynchronous. If the synchronous follower becomes unavailable or slow, one of the asynchronous followers is made synchronous. In practice, if you enable synchronous replication on a database, it usually means semi-synchronous. Synchronous is impractical because the leader must block all writes and wait until the synchronous replica is available again. Often, leader-based replication is configured to be completely asynchronous. In this case, if the leader fails and is not recoverable, any writes that have not yet been replicated to followers are lost. This means that a write is not guaranteed to be durable, even if it has been confirmed to the client. However, a fully asynchronous configuration has the advantage that the leader can continue processing writes, even if all of its followers have fallen behind. Granularity of Replication Logs Statement-based replication: Deprecated. In the simplest case, the leader logs every write request (statement) that it executes and sends that statement log to its followers. For a relational database, this means that every INSERT, UPDATE, or DELETE statement is forwarded to followers, and each follower parses and executes that SQL statement as if it had been received from a client. Statements may be nondeterministic or have side effects, which causes replications diverge. Write-ahead log (WAL) shipping: the log is an append-only sequence of bytes containing all writes to the database. We can use the exact same log to build a replica on another node. The disadvantage is that it makes replication closely coupled to the storage engine. A newer software version usually had format compatibility issue, which requires downtime for upgrading. Logical (row-based) log replication: A logical log for a relational database is usually a sequence of records describing writes to database tables at the granularity of a row. Replication architecturesThere are three main approaches to replication and each approach has advantages and disadvantages. Single-leader replication is popular because it is fairly easy to understand and there is no conflict resolution to worry about. Multi-leader and leaderless replication can be more robust in the presence of faulty nodes, network interruptions, and latency spikes—at the cost of being harder to reason about and providing only very weak consistency guarantees. Single-leader replication One of the replicas is designated the leader. Write requests of clients can only go to leader, which first writes the new data to its local storage. The other replicas are known as followers . Whenever the leader writes new data to its local storage, it also sends the data change to all of its followers. Each follower takes the log from the leader and updates its local copy of the database accordingly. When a client wants to read from the database, it can query either the leader or any of the followers. However, writes are only accepted on the leader. Multi-leader replicationClients send each write to one of several leader nodes, any of which can accept writes. The leaders send streams of data change events to each other and to any follower nodes. Multi-leader replication has advantage on performance, tolerance of datacenter outages, tolerance of network problems.However, it rarely makes sense to use a multi-leader setup within a single datacenter, because the benefits rarely outweigh the added complexity: the same data may be concurrently modified in two different datacenters, and those write conflicts must be resolved. Here are some common use cases for multi-leader replication: Multi-datacenter operation: Within each datacenter, regular leader-follower replication is used; between datacenters, each datacenter’s leader replicates its changes to the leaders in other datacenters. Clients with offline operation: you have an application that needs to continue to work with multiple devices while they are disconnected from the internet.In this case, every device has a local database that acts as a leader (it accepts write requests), and there is an asynchronous multi-leader replication process (sync) between the replicas on all of your devices. Collaborative editing: When one user edits a document, the changes are instantly applied to their local replica (the state of the document ) and asynchronously replicated to the server and any other users who are editing the same document. Leaderless replicationAlso called Dynamo-style: allow any replica to directly accept writes from clients. In some leaderless implementations, the client directly sends its writes to several replicas, while in others, a coordinator node does this on behalf of the client. However, unlike a leader database, that coordinator does not enforce a particular ordering of writes. Version numbers are used to determine which value is newer. quorum reads and writes tries to solve the issue of reading stale data: if there are n replicas, every write must be confirmed by w nodes(synchronized) to be considered successful, and we must query at least r nodes for each read. As long as there is an overlap, that is, w + r &gt; n, we expect to get an up-to-date value when reading. With a smaller w and r you are more likely to read stale values, but this configuration allows lower latency and higher availability. However, even with w + r &gt; n, there are likely to be edge cases where stale values are returned, e.g., sloppy quorum, two writes occur concurrently, a write happens concurrently. Thus, Dynamo-style databases are generally optimized for use cases that can tolerate eventual consistency. How to ensure that eventually all the data is copied to every replica? There are two ways: Read repair: When a client makes a read from several nodes in parallel, it can detect any stale responses. The client then writes the newer value back to the replica that has stale data.This approach works well for values that are frequently read. However, values that are rarely read may be missing from some replicas and thus have reduced durability. Anti-entropy process: a background process that constantly looks for differences in the data between replicas and copies any missing data from one replica to another. Unlike the replication log in leader-based replication, this anti-entropy process does not copy writes in any particular order, and there may be a significant delay before data is copied. Node FailuresIf a follower fails, we can use the Catch-up strategy. On its local disk, each follower keeps a log of the data changes it has received from the leader. If a follower crashes and is restarted, or if the network recovers from a temporary failure, it can connect to the leader and request all the data changes that occurred during the time when the follower was disconnected. When it has applied these changes, it has caught up to the leader and can continue receiving a stream of data changes as before. If a leader fails, we can use the Failover strategy: one of the followers needs to be promoted to be the new leader, clients need to be reconfigured to send their writes to the new leader, and the other followers need to start consuming data changes from the new leader. An automatic failover process usually consists of the following steps: Determining that the leader has failed. There is no foolproof way of detecting if a node is crashed or disconnected from the network, so most systems simply use a timeout: nodes frequently bounce heartbeat message back and forth between each other, and if a node doesn’t respond for some period of time, it is assumed to be dead. Choosing a new leader. This could be done through an election process, or a new leader could be appointed by a previously elected controller node. The best candidate for leadership is usually the replica with the most up-to-date data changes from the old leader. Reconfiguring the system to use the new leader. Clients now need to send their write requests to the new leader. The system needs to ensure that the old leader becomes a follower and recognizes the new leader, if the old leader comes back. Challenges with Replication LagIn an asynchronous architecture, client may see outdated information if the replica has fallen behind. If you stop writing to the database and wait a while, replicas will eventually catch up and become consistent with each other, which is known as eventual consistency. When working with an eventually consistent system, it is worth thinking about how the application behaves if the replication lag increases to several minutes or even hours. If the answer is “no problem,” that’s great. However, if the result is a bad experience for users, it’s important to design the system to provide a stronger guarantee. Reading Your Own WritesWhen new data is submitted, it must be sent to the leader, but when the user views the data, it can be read from a follower. the new data may not yet have reached the replica. To the user, it looks as though the data they submitted was lost read-after-write consistency: This is a guarantee that if the user reloads the page, they will always see any updates they submitted themselves. It makes no promises about other users: other users’ updates may not be visible until some later time. Implementations in leader-based architecture: If only a few things in the application are potentially editable by the user, when reading something that the user may have modified, read it from the leader; otherwise, read it from a follower Use criteria to decide whether to read from the leader, i.e, for one minute after the last update, make all reads from the leader The client can remember the timestamp of its most recent write. If a replica is not sufficiently up to date, query another replica or wait until the replica has caught up. Monotonic ReadsWhen reading from asynchronous followers is that it’s possible for a user to see things moving backward in time because of reading an out-of-date replica. Monotonic reads: if one user makes several reads in sequence, they will not read older data after having previously read newer data. Implementation: each user always makes their reads from the same replica, i.e., the replica can be chosen based on a hash of the user ID. Consistent Prefix Readsconsistent prefix reads: if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order. One implementation is to make sure that any writes that are causally related to each other are written to the same partition. Concurrency Write ConflictsConcurrency Write issues are inherent in multi-leader and leaderless replication approaches: because they allow multiple writes to happen concurrently, conflicts may occur. Conflict avoidanceIf the application can ensure that all writes for a particular record go through the same leader, then conflicts cannot occur, i.e, route user write requests to the same leader based on the hash of its user id. Converging toward a consistent stateApproaches that are dangerously prone to data loss: Give each write a unique ID (e.g., a timestamp, a long random number, a UUID, or a hash of the key and value), pick the write with the highest ID as the winner, and throw away the other writes. If a timestamp is used, this technique is known as last write wins (LWW) Give each replica a unique ID, and let writes that originated at a higher numbered replica always take precedence over writes that originated at a lower numbered replica. There are some situations, such as caching, in which lost writes are perhaps acceptable. If losing data is not acceptable, LWW is a poor choice for conflict resolution. The only safe way of using a database with LWW is to ensure that a key is only written once and thereafter treated as immutable, thus avoiding any concurrent updates to the same key. For example, a recommended way of using Cassandra is to use a UUID as the key, thus giving each write operation a unique key Other approaches are: merge the values together, e.g., order them alphabetically and then concatenate them Record the conflict in an explicit data structure that preserves all information, and write application code that resolves the conflict at some later time (perhaps by prompting the user). Reference [1] Designing Data-Intensive Applications, Chapter 05: Replication","categories":[{"name":"Reading Notes","slug":"Reading-Notes","permalink":"https://blog-pika.github.io/categories/Reading-Notes/"},{"name":"Designing Data-Intensive Applications","slug":"Reading-Notes/Designing-Data-Intensive-Applications","permalink":"https://blog-pika.github.io/categories/Reading-Notes/Designing-Data-Intensive-Applications/"}],"tags":[{"name":"Replication","slug":"Replication","permalink":"https://blog-pika.github.io/tags/Replication/"},{"name":"Distributed System","slug":"Distributed-System","permalink":"https://blog-pika.github.io/tags/Distributed-System/"},{"name":"DDIA","slug":"DDIA","permalink":"https://blog-pika.github.io/tags/DDIA/"}]}]}