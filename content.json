{"meta":{"title":"The Code and Sixpence","subtitle":null,"description":null,"author":"Developer Pikachu","url":"https://blog-pika.github.io","root":"/"},"pages":[{"title":"categories","date":"2019-08-07T03:05:45.000Z","updated":"2019-08-07T03:05:56.727Z","comments":true,"path":"categories/index.html","permalink":"https://blog-pika.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-08-07T02:54:50.000Z","updated":"2019-08-07T03:04:12.703Z","comments":true,"path":"tags/index.html","permalink":"https://blog-pika.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"6.824 Lab 3A - Key/value service without log compaction","slug":"CourseProject/6-824-lab-3a","date":"2019-09-05T20:41:57.606Z","updated":"2019-09-08T21:09:50.903Z","comments":true,"path":"2019/09/05/CourseProject/6-824-lab-3a/","link":"","permalink":"https://blog-pika.github.io/2019/09/05/CourseProject/6-824-lab-3a/","excerpt":"This blog is my notes for MIT 6.824 Lab 3A: Key/value service without log compaction.","text":"This blog is my notes for MIT 6.824 Lab 3A: Key/value service without log compaction. Design OverviewThe overview is as follow: The steps are as follow: The Client uses its Clerk to send GET/PUT/APPEND RPC to leader server. If a follower server receives client request, it should reply with isLeader = false, then the client can try another server until it found the leader server. If the request is not successful, e.g., due to server crashes, partition, leadership lost, network issues, the client should retry until receives a successful reply. When the leader KV-service receives RPC args from client, it checks if the request is duplicated. If yes, it replies with the data in the in-memory DB. If the request is not duplicated, leader KV-service submits this log to Raft-Service and is blocked a golang channel. It will be waken up once notified by ApplyDaemon. Leader Raft-Service duplicates logs to other followers and once the consensus is reached, it sends committed log to the ApplyDaemon via a golang channel. Once ApplyDaemon receives logs from Raft-Service, it applies the logs to in-memory database. (All the alive servers should do this). The leader KV-service replies RPC with corresponding information. Since the client need to retry the request, we should use clientId + requestID to distinguish every different request. If the server finds out that it loses its leadership, by checking isLeader and term returned by Raft, it should stop execution and replies with isLeader = false. Client only sends new request when the old request is acknowledged. Therefore, every request except the current one are committed and durable in Raft. Raft takes care of consensus, crash and partition. FAQ Q: Are KV-Service and Raft-Server paired? In other words, a server in this Lab has both a KV-service and a Raft-Service? A: Yes Q: How to handle concurrent requests from different clients? A: All the requests are submitted to the Raft leader, which has a single thread-safe entry: kv.rf.Start(args.getCommand()). Therefore, all the concurrent requests will be serialized inside Raft leader. Also, this sequence will be maintained inside KV-server afterwards for processing. Q: How to guarantee that every request is executed only once. If every request is executed at-least-once and at-most-once, then it is executed exactly-once. at-least-once: If the client does not receive the ack of a request from KV-server, it will resend the request infinitely until success. Also, as long as the request is committed, Raft guarantees logs’ durability in spite of servers crash or leadership change. at-most-once: Every Request comes with a ClientId and a RequestId. We can check if a request has been executed by checking ClientId + RequestId. Therefore, every request will only be executed at-most-once. Q: Can KV-Server accepts the appliedLog from the follower Raft-Server? This question is identical to do we need to check if the server is still leader when we receive log from Raft. A: No, the followers may lag behind. If the leader has accepted some write requests and acks the client, a follower’s return data may be stale, which violates strong consistency. Some other systems may relax the consistency model for better performance, e.g., Zookeeper can read from the follower. Q: In KV-server, for every client, do we need to memorize all the logs (or at least the latest log) we have applied? A: No. Remember the requestId of the latest applied log is good enough. If the request is stale, i.e., args.requestId &lt;= appliedId[args.clientId], we don’t need to submit it to the raft server. If it is a write request, ignore it; if it is a read request, get the value from KV-Server in-memory key-value pair. Q: When applyDaemon goroutine receives log from Raft, it should notify the goroutine of KV service. What can we use to identify a log? A: ClientId + RequestId may do the trick. But using the index returned by Raft should lead to more concise and less error-prone code. Q: If we use the index returned by kv.rf.Start(log), we may face the following race condition: A: KV server will wait and timeouts, so client will issue a new request, so this race condition will not affect correctness. Also, the time Raft need to reach consensus is relatively long (compared to single node execution), normally this race condition won’t happen. Q: In the test file, why do we need both end and endname? A: This is similar to file and file name. The server has end and the config manages end by endname. Q: In the config.go file, why is the endnames a two-dimension array? A: Endname is unidirectional. We need to record every unidirectional socket for server_i -&gt; server_j, therefore a two-dimension array is needed. Q: In the config.go file, what does to stands for in functions such as func (cfg *config) connectUnlocked(i int, to []int)? A: This is not a good name. To stands for a partition of servers.","categories":[{"name":"Project","slug":"Project","permalink":"https://blog-pika.github.io/categories/Project/"}],"tags":[{"name":"6.824","slug":"6-824","permalink":"https://blog-pika.github.io/tags/6-824/"}]},{"title":"Zookeeper","slug":"Paper/Zookeeper","date":"2019-08-21T07:00:00.000Z","updated":"2019-09-08T21:10:24.820Z","comments":true,"path":"2019/08/21/Paper/Zookeeper/","link":"","permalink":"https://blog-pika.github.io/2019/08/21/Paper/Zookeeper/","excerpt":"This blog is my reading notes for Zookeeper. We can learn from this paper how to use consensus algorithm to build replicated services with high read throughput.","text":"This blog is my reading notes for Zookeeper. We can learn from this paper how to use consensus algorithm to build replicated services with high read throughput. Data Model and APIZooKeeper provides to its clients the abstraction of a set of data nodes (znodes), organized as a file system with only full data reads and writes API. Every znode maps to abstractions of the client application, typically corresponding to meta-data used for coordination purposes. All except for ephemeral znodes, can have children: Regular: Clients manipulate regular znodes by creating and deleting them explicitly Ephemeral: Clients create such znodes, and they either delete them explicitly, or let the system remove them automatically when the session that creates them terminates (deliberately or due to a failure) A client connects to ZooKeeper and initiates a session. Sessions have an associated timeout. ZooKeeper considers a client faulty if it does not receive anything from its session for more than that timeout. A session ends when clients explicitly close a session handle or ZooKeeper detects that a clients is faulty. When creating a new znode, a client can set a sequential ﬂag. Nodes created with the sequential flag set have the value of a monotonically increasing counter appended to its name. ZooKeeper also implements watches to allow clients to receive timely notifications of changes without requiring polling. When a client issues a read operation with a watch flag set, the operation completes as normal except that the server promises to notify the client when the information returned has changed. Watches are one-time triggers associated with a session; they are unregistered once triggered or the session closes. Watches indicate that a change has happened, but do not provide the change. The APIs are as follow: create(path, data, flags): Creates a znode with path name path, stores data[] in it, and returns the name of the new znode. flags can be regular/ephemeral + sequential flag delete(path, version): Deletes the znode path if that znode is at the expected version exists(path, watch): Returns true if the znode with path name path exists, and returns false otherwise. The watch flag enables a client to set a watch on the znode getData(path, watch): Returns the data and meta-data, such as version information, associated with the znode. ZooKeeper does not set the watch if the znode does not exist setData(path, data, version): Writes data[] to znode path if the version number is the current version of the znode getChildren(path, watch): Returns the set of names of the children of a znode sync(path): Waits for all updates pending at the start Zookeeper provides a consistency model with ordering guarantees tailed for read heavy scenarios: Linearizable writes order: all requests that update the state of ZooKeeper are serializable and respect precedence FIFO client order: all requests from a given client are in the order that they were sent by the client. Notification ordering: if a client is watching for a change, the client will see the notification event before it sees the new state of the system after the change is made It also provide additional two guarantees: liveness and guarantees: if a majority of ZooKeeper servers are active and communicating the service will be available durability: if the ZooKeeper service responds successfully to a change request, that change persists across any number of failures as long as a quorum of servers is eventually able to recover. ArchitectureThe components of Zookeeper are as follow: WriteWrite needs to be duplicated among servers, so it is expensive. When a write request comes: write requests are forwarded to leader leader calculates what the state of the system will be when the write is applied and transforms it into a transaction that captures this new state. followers receive message proposals consisting of state changes from the leader and agree upon state changes. ZAB uses by default simple majority quorums to decide on a proposal. ZAB guarantees that changes broadcast by a leader are delivered in the order they were sent and all changes from previous leaders are delivered to an established leader before it broadcasts its own changes. ReadA read operation is fast since it is read local data. The drawback is that a read operation may return a stale value because the local server may lag behind. For application requires latest data from the requesting server, Zookeeper implemented sync. The FIFO order guarantee of client operations together with the global guarantee of sync enables the result of the read operation to reflect any changes that happened before the sync was issued. We simply place the sync operation at the end of the queue of requests between the leader and the server executing the call to sync (not all servers). In order for this to work, the follower must be sure that the leader is still the leader. If there are pending transactions that commit, then the server does not suspect the leader. If the pending queue is empty, the leader needs to issue a null transaction to commit and orders the sync after that transaction. When session transfer from one server to another server, it should not see data in the past. Zookeeper client records the last transaction it has seen (zxid). If the client connects to a new server, that new server ensures that its view of the ZooKeeper data is at least as recent as the view of the client by checking the last zxid of the client against its last zxid. If the client has a more recent view than the server, the server does not reestablish the session with the client until the server has caught up. The client is guaranteed to be able to find another server that has a recent view of the system since the client only sees changes that have been replicated to a majority of the ZooKeeper servers. RecoveryWhen a ZooKeeper server recovers from a crash, it needs to recover snapshot + replay all delivered messages to recover state. ZooKeeper uses periodic snapshots and only requires redelivery of messages since the start of the snapshot. We call ZooKeeper snapshots fuzzy snapshots since we do not lock the ZooKeeper state to take the snapshot. Since the resulting fuzzy snapshot may have applied some subset of the state changes delivered during the generation of the snapshot, the result may not correspond to the state of ZooKeeper at any point in time. However, since state changes are idempotent, we can apply them twice as long as we apply the state changes in order. Examples of primitivesZooKeeper API can be used to implement more powerful primitives, e.g., configuration management, rendezvous, group membership, lock, R/W lock, barrier. FAQHere is the FAQ from MIT 6.824 Q: What does Linearizable mean? A: In a linearizable system, as soon as one client successfully completes a write, all clients reading from the database must be able to see the value just written. The value read is the most recent, up-to-date value, and doesn’t come from a stale cache or replica. In other words, linearizability is a recency guarantee. For detail please refer to Designing Data Intensive Application, Chapter 9.2, Linearizability. Q: What does pipeline mean? A: MIT 6.824 FAQ: Zookeeper “pipelines” the operations in the client API (create, delete, exists, etc). What pipelining means here is that these operations are executed asynchronously by clients. The client calls create, delete, sends the operation to Zookeeper and then returns. At some point later, Zookeeper invokes a callback on the client that tells the client that the operation has been completed and what the results are. This asynchronous interface allow a client to pipeline many operations: after it issues the first, it can immediately issue asecond one, and so on. This allows the client to achieve high throughput; it doesn’t have to wait for each operation to complete before starting a second one. A worry with pipelining is that operations that are in flight might be re-ordered, which would cause the problem that the authors to talk about in 2.3. If a the leader has many write operations in flight followed by write to ready, you don’t want those operations to be re-ordered, because then other clients may observe ready before the preceding writes have been applied. To ensure that this cannot happen, Zookeeper guarantees FIFO for client operations; that is the client operations are applied in the order they have been issued. Q: In the case of read requests, a server simply reads the state of the local database, will we get stale data? A: Yes, we may read stale data. If we need the latest data, we can use sync. Q: Each read request is processed and tagged with a zxid that corresponds to the last transaction seen by the server. What does transaction stands for? A: A write request. Reference ZooKeeper: Wait-free coordination for Internet-scale systems 6.824 Schedule: Spring 2018 Designing Data Intensive Application, Chapter 9.2, Linearizability","categories":[{"name":"Paper","slug":"Paper","permalink":"https://blog-pika.github.io/categories/Paper/"}],"tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://blog-pika.github.io/tags/Zookeeper/"},{"name":"6.824","slug":"6-824","permalink":"https://blog-pika.github.io/tags/6-824/"}]},{"title":"Spinnaker","slug":"Paper/Spinnaker","date":"2019-08-20T03:12:09.000Z","updated":"2019-09-08T21:10:14.562Z","comments":true,"path":"2019/08/19/Paper/Spinnaker/","link":"","permalink":"https://blog-pika.github.io/2019/08/19/Paper/Spinnaker/","excerpt":"This blog is my reading notes for Spinnaker. We can learn from this paper how to use consensus algorithm (Paxos) to do replication in database.","text":"This blog is my reading notes for Spinnaker. We can learn from this paper how to use consensus algorithm (Paxos) to do replication in database. ProsSpinnaker features key-based range partitioning, 3-way replication, and a transactional get-put API with the option to choose either strong or timeline consistency on reads. Spinnaker will be available for reads and writes as long a majority of its replicas are alive. The performance can be competitive with alternatives that provide weaker consistency guarantees. Compared to an eventually consistent datastore, Spinnaker can be as fast or even faster on reads and only 5% to 10% slower on writes. ConsOnly suitable in single datacenter and cannot tolerate network partition Data Model and APIData Model is almost the same as BigTable. Besides, each API call is executed as a single-operation transaction. 1234567891011121314get(key, colname, consistent)// Read a column value and its version number from a row. The setting of the ‘consistent’ ﬂag is used to choose the consistency level. Setting it to ‘true’ chooses strong consistency, and the latest value is always returned. Setting it to ‘false’ chooses timeline consistency, and a possibly stale value is returned in exchange for better performance.put(key, colname, colvalue)// Insert a column value into a row.delete(key, colname)// Delete a column from a row.conditionalPut(key, colname, value, v)// Insert a new column value into a row only if the column’s current version number is equal to ‘v’. Otherwise, an error is returned.conditionalDelete(key, colname, v)//Like conditional put but for delete. ArchitectureThe architecture of Spinnaker is also similar to BigTable. However, they use different ways to make logs and data fault-tolerant: BigTable: store data and logs on GFS Spinnaker: use Paxos to replicate logs to multiple nodes ReplicationLike Bigtable, Spinnaker distributes the rows of a table across its cluster using range partitioning. Each node is assigned a base key range, which is replicated on the next N − 1 nodes. Each group of nodes involved in replicating a key range is denoted as a cohort. Figure below shows a Spinnaker cluster with 5 nodes and N = 3. Each cohort has an elected leader, with the other 2 nodes acting as followers. The replication protocol has two phases: a leader election phase, followed by a quorum phase where the leader proposes a write and the followers accept it. The steps of Spinnaker’s replication protocol in steady state are as follow: When a client submits a write W, it always gets routed to the leader of the affected key range The leader appends a log record for W to its log and then initiates a log force to disk The leader appends W to its commit queue and sends a propose message for W to its followers When the followers receive the propose message, they force a log record for W to disk, append W to their commit queue, and reply with an ack to the leader After the leader gets an ack from the majority, it applies W to its memtable, effectively committing W The leader returns a response to the client. Periodically, the leader sends an asynchronous commit message to the followers asking them to apply all pending writes up to a certain LSN to their memtable. Strongly consistent reads are always routed to the cohort’s leader, so they are guaranteed to see the latest value for W. Timeline consistent reads can be routed to any node in the cohort, so they may see a stale value for W until its commit message is processed RecoveryLet f.cmt = follower’s last committed LSN and f.lst = the follower’s last LSN in its log. The recovery of a follower proceeds in two phases: local recovery: simply replay log between snapshot and f.cmt to recover the memtablecatch up: the logs between f.cmt and f.lst may or may not have been committed by the leader, so the follower has to consult the leader The recovery of leader consists of re-election. The new leader may be not up-to-date, so it need to truncate the logs. However, since different key ranges write to the same log file (same as BigTable), Spinnaker uses skip-list to do logical truncation. FAQThe lecture of MIT has FAQ: Spinnaker FAQ Q: What is a shared write-ahead log? A: Same as BigTable, one node only has one log file to improve performance. Different key ranges write to the same log file. Q: Periodically, the leader in Spinnaker sends an asynchronous commit message to the followers. Is this design better than sending the commit message as soon as the leader commits the log? Q: The paper says “A conditional put is guaranteed to have the same outcome on each node of the cohort because writes are executed in LSN order within a cohort.” If using normal put, will the writes be applied out of order? Q: The paper says: “At the end of the catch up phase, the leader momentarily blocks new writes to ensure that that the follower is fully caught up.” I don’t think blocking the writes is necessary. Reference Using Paxos to Build a Scalable, Consistent, and Highly Available Datastore 6.824 Schedule: Spring 2018","categories":[{"name":"Paper","slug":"Paper","permalink":"https://blog-pika.github.io/categories/Paper/"}],"tags":[{"name":"Paxos","slug":"Paxos","permalink":"https://blog-pika.github.io/tags/Paxos/"},{"name":"6.824","slug":"6-824","permalink":"https://blog-pika.github.io/tags/6-824/"}]},{"title":"Consistency Level","slug":"Distributed-System-101/Consistency-Level","date":"2019-08-19T15:41:09.000Z","updated":"2019-08-20T03:11:50.203Z","comments":true,"path":"2019/08/19/Distributed-System-101/Consistency-Level/","link":"","permalink":"https://blog-pika.github.io/2019/08/19/Distributed-System-101/Consistency-Level/","excerpt":"","text":"Strongly-consistent systems are easier to program because they behave like a non-replicated systems. Building applications on top of eventually-consistent systems is generally more difficult because the programmer must consider scenarios that couldn’t come up in a non-replicated system. On the other hand, one can often obtain higher performance from weaker consistency models. Timeline ConsistencyA form of relaxed consistency that can provide faster reads at the expense of visibly anomalous behavior. All replicas apply writes in the same order (clients send writes to the leader, and the leader picks an order and forwards the writes to the replicas). Clients are allowed to send reads to any replica, and that replica replies with whatever data it current has. The replica may not yet have received recent writes from the leader, so the client may see stale data. If a client sends a read to one replica, and then another, the client may get older data from the second read. Unlike strong consistency, timeline consistency makes clients aware of the fact that there are replicas, and that the replicas may differ.","categories":[],"tags":[]},{"title":"BigTable","slug":"Paper/BigTable","date":"2019-08-16T16:46:00.000Z","updated":"2019-08-20T03:16:06.082Z","comments":true,"path":"2019/08/16/Paper/BigTable/","link":"","permalink":"https://blog-pika.github.io/2019/08/16/Paper/BigTable/","excerpt":"","text":"This blog is my reading notes for Google Bigtable. Pros and ConsAPIClient applications can write or delete values in Bigtable, look up values from individual rows, or iterate over a subset of the data in a table. Bigtable supports single-row transaction. 12345678910// Writing to Bigtable// Open the tableTable *T = OpenOrDie(\"/bigtable/web/webtable\");// Write a new anchor and delete an old anchorRowMutation r1(T, \"com.cnn.www\");r1.Set(\"anchor:www.c-span.org\", \"CNN\");r1.Delete(\"anchor:www.abc.com\");Operation op; Appaly(&amp;op, &amp;r1); 123456789101112// Reading from BigtableScanner scanner(T);ScanStream *stream;stream = scanner.FetchColumnFamily(\"anchor\");stream-&gt;SetReturnAllVersions();scanner.Lookup(\"com.cnn.www\");for (; !stream-&gt;Done(); stream-&gt;Next()) &#123; printf(\"%s %s %lld %s\\n\", scanner.RowName(), stream-&gt;ColumnName(), stream-&gt;MicroTimestamp(), stream-&gt;Value());&#125; Data ModelBigtable is a sparse, distributed, persistent multidimensional sorted map. The map is indexed by a row key, column key, and a timestamp, each value in the map is an uninterpreted array of bytes. Every read or write of data under a single row key is atomic. The row range for a table is dynamically partitioned, which is called tablet and is the unit of distribution and load balancing. Column keys are grouped into sets called column families, which form the basic unit of access control. All data stored in a column family is usually of the same type, which can be indexed with: family:qualifier. Each cell in a Bigtable can contain multiple versions of the same data; these versions are indexed by timestamp. The client can specify either that only the last n versions of a cell be kept, or that only new-enough versions be kept. The out-dated versions are GC automatically. ArchitectureUnder the hook, Bigtable uses several other Google infrastructure as below GFS: store logs and data files Borg: schedule jobs, manage resources, deal with machine failures and monitoring machine status Chubby: avoid multiple master, store the bootstrap location, discover tablet servers and finalize tablet server deaths For the components in BigTable: Master: responsible for assigning tablets to tablet servers, detecting the addition and expiration of tablet servers, balancing tablet-server load, and garbage collection of files in GFS tablet server: manages a set of tablets clients: every client has a client library, which caches tablet locations, so a client can communicate directly with tablet servers for reads and writes When a write operation arrives at a tablet server, the server checks that it is well-formed, and that the sender is authorized to perform the mutation. A valid mutation is written to the commit log. After the write has been committed, its contents are inserted into the memtable. When a read operation arrives at a tablet server, it is executed on a merged view of the sequence of SSTables and the memtable. Data Storage EngineBigTable used the data structure of SSTable, memtable and Bloomfilter. Also, it uses column storage and compression for the column. For detail you can refer to Designing Data-Intensive Applications, Chapter 03: Storage and Retrieval. Service DiscoveryChubby is the key to discover all the servers and tablet information in BigTable. When a tablet server/master server starts, it creates, and acquires an exclusive lock on Chubby directory. Therefore, a server can find the information and status of other servers by asking Chubby. Chubby if also the root for a three-level hierarchy to store tablet location information. first level: Chubby that contains the location of the root tablet second level: Root tablet contains the location of all tablets in a special METADATA table third level: each METADATA tablet contains the location of a set of user tablets Load BalanceMost Load balancing are done by the master except one: Master: a table is created or deleted, two existing tablets are merged to form one larger tablet Tablet Server: an existing tablet is split into two smaller tablets The master is able to keep track of these changes because it initiates all. Tablet splits are treated specially since they are initiated by a tablet server. The tablet server commits the split by recording information for the new tablet in the METADATA table. When the split has committed, it notifies the master. In case the split notification is lost (either because the tablet server or the master died), the master detects the new tablet when it asks a tablet server to load the tablet that has now split. The tablet server will notify the master of the split, because the tablet entry it finds in the METADATA table will specify only a portion of the tablet that the master asked it to load. Fault ToleranceGFS and Chubby are fault-tolerant, which can simplify the design of BitTable. GFS takes care of the fault-tolerance of file and Bigtable uses Chubby to keep track of servers. If Chubby fails, BigTable will soon be unavailable. For the master, it kills itself if its Chubby session expires. Since master only cares about load balancing and meta data, the crash of master is not a big deal, Borg can simply start a new master: he master grabs a unique master lock in Chubby, which prevents concurrent master instantiations The master scans the servers directory in Chubby to find the live servers. The master communicates with every live tablet server to discover what tablets are already assigned to each server. The master scans the METADATA table to learn the set of tablets. What if the tablet server is unavailable? The master is responsible for detecting when a tablet server is no longer serving its tablets. If a tablet server is unavailable, it loses its exclusive lock. The master periodically asks each tablet server for the status of its lock. If a tablet server reports that it has lost its lock, or if the master was unable to reach a server during its last several attempts, the master attempts to acquire an exclusive lock on the server’s file. If the master is able to acquire the lock, then Chubby is live and the tablet server is either dead or having trouble reaching Chubby, so the master ensures that the tablet server can never serve again by deleting its server file. Once a server’s file has been deleted, the master can move all the tablets that were previously assigned to that server into the set of unassigned tablets. FAQ Q: When a server crashes, will the data in memetable lost? A: No, the write operation writes the logs in GFS first, so the memory data can be recovered from logs. Q: Will Chubby file or Root Tablet be the hotspot? A: No. Client library does aggressive pre-fetching and caching so the root tablet file is not accessed that frequently. Q: How does this work: The only mutable data structure that is accessed by both reads and writes is the memtable. To reduce contention during reads of the memtable, we make each memtable row copy-on-write and allow reads and writes to proceed in parallel. A: ??? Q: How can two log writing threads protects mutations from GFS latency spikes? A: ??? Q: How can copy-on-write on each memtable row allows reads and writes to proceed in parallel? A:??? GFS可能出现重复记录或者padding，Bigtable如何处理这种情况使得对外提供强一致性模型？ 为什么Bigtable设计成Root、Meta、User三级结构，而不是两级或者四级结构？ 读取某一行用户数据，最多需要几次请求？分别是什么？ 如何保证同一个tablet不会被多台机器同时服务？ Tablet在内存中的数据结构如何设计？ 如何设计SSTable的存储格式？ minor、merging、major这三种compaction有什么区别？ Tablet Server的缓存如何实现？ 如果tablet出现故障，需要将服务迁移到其它机器，这个过程需要排序操作日志。如何实现？ 如何使得tablet迁移过程停服务时间尽量短？ tablet分裂的流程是怎样的？ tablet合并的流程是怎样的？ Reference Bigtable: A Distributed Storage System for Structured Data Youtube: BigTable: A Distributed Structured Storage System 有哪些分布式数据库书籍或论文比较好？ Designing Data-Intensive Applications, Chapter 03: Storage and Retrieval","categories":[{"name":"Paper","slug":"Paper","permalink":"https://blog-pika.github.io/categories/Paper/"}],"tags":[]},{"title":"Data Structures That Power Your Database","slug":"Distributed-System-101/Data-Structures-That-Power-Your-Database","date":"2019-08-15T07:00:00.000Z","updated":"2019-09-08T21:10:31.595Z","comments":true,"path":"2019/08/15/Distributed-System-101/Data-Structures-That-Power-Your-Database/","link":"","permalink":"https://blog-pika.github.io/2019/08/15/Distributed-System-101/Data-Structures-That-Power-Your-Database/","excerpt":"AbstractThis blog is my reading notes for Designing Data-Intensive Applications, Chapter 05: Replication. Replication means keeping a copy of the same data on multiple machines that are connected via a network. It has following benefits: High availability: Keeping the system running, even when one/several machines goes down Latency: Placing data geographically close to users, so that users can interact with it faster Scalability: Being able to handle a higher volume of reads than a single machine could handle, by performing reads on replicas","text":"AbstractThis blog is my reading notes for Designing Data-Intensive Applications, Chapter 05: Replication. Replication means keeping a copy of the same data on multiple machines that are connected via a network. It has following benefits: High availability: Keeping the system running, even when one/several machines goes down Latency: Placing data geographically close to users, so that users can interact with it faster Scalability: Being able to handle a higher volume of reads than a single machine could handle, by performing reads on replicas However, keeping data consistent between replicas are not as simple as it looks. We need to consider the following issues: node failures; unreliable networks; and tradeoffs around replica consistency, durability, availability, and latency. Let’s look at them one by one: should we use synchronous or asynchronous? which granularity of Replication Logs should we use? which kind of replication architecture: Single-leader, multiple-leader, or leaderless? should we provide consistency stronger than eventual consistency? How to handle node failure and unreliable network? How to handle concurrency write conflicts? Synchronous vs Asynchronous synchronous: the leader waits until follower has confirmed that it received the write before reporting success to the user, and before making the write visible to other clients. asynchronous: the leader sends the message, but doesn’t wait for a response from the follower. semi-synchronous: one of the followers is synchronous, and the others are asynchronous. If the synchronous follower becomes unavailable or slow, one of the asynchronous followers is made synchronous. In practice, if you enable synchronous replication on a database, it usually means semi-synchronous. Synchronous is impractical because the leader must block all writes and wait until the synchronous replica is available again. Often, leader-based replication is configured to be completely asynchronous. In this case, if the leader fails and is not recoverable, any writes that have not yet been replicated to followers are lost. This means that a write is not guaranteed to be durable, even if it has been confirmed to the client. However, a fully asynchronous configuration has the advantage that the leader can continue processing writes, even if all of its followers have fallen behind. Granularity of Replication Logs Statement-based replication: Deprecated. In the simplest case, the leader logs every write request (statement) that it executes and sends that statement log to its followers. For a relational database, this means that every INSERT, UPDATE, or DELETE statement is forwarded to followers, and each follower parses and executes that SQL statement as if it had been received from a client. Statements may be nondeterministic or have side effects, which causes replications diverge. Write-ahead log (WAL) shipping: the log is an append-only sequence of bytes containing all writes to the database. We can use the exact same log to build a replica on another node. The disadvantage is that it makes replication closely coupled to the storage engine. A newer software version usually had format compatibility issue, which requires downtime for upgrading. Logical (row-based) log replication: A logical log for a relational database is usually a sequence of records describing writes to database tables at the granularity of a row. Replication architecturesThere are three main approaches to replication and each approach has advantages and disadvantages. Single-leader replication is popular because it is fairly easy to understand and there is no conflict resolution to worry about. Multi-leader and leaderless replication can be more robust in the presence of faulty nodes, network interruptions, and latency spikes—at the cost of being harder to reason about and providing only very weak consistency guarantees. Single-leader replication One of the replicas is designated the leader. Write requests of clients can only go to leader, which first writes the new data to its local storage. The other replicas are known as followers . Whenever the leader writes new data to its local storage, it also sends the data change to all of its followers. Each follower takes the log from the leader and updates its local copy of the database accordingly. When a client wants to read from the database, it can query either the leader or any of the followers. However, writes are only accepted on the leader. Multi-leader replicationClients send each write to one of several leader nodes, any of which can accept writes. The leaders send streams of data change events to each other and to any follower nodes. Multi-leader replication has advantage on performance, tolerance of datacenter outages, tolerance of network problems.However, it rarely makes sense to use a multi-leader setup within a single datacenter, because the benefits rarely outweigh the added complexity: the same data may be concurrently modified in two different datacenters, and those write conflicts must be resolved. Here are some common use cases for multi-leader replication: Multi-datacenter operation: Within each datacenter, regular leader-follower replication is used; between datacenters, each datacenter’s leader replicates its changes to the leaders in other datacenters. Clients with offline operation: you have an application that needs to continue to work with multiple devices while they are disconnected from the internet.In this case, every device has a local database that acts as a leader (it accepts write requests), and there is an asynchronous multi-leader replication process (sync) between the replicas on all of your devices. Collaborative editing: When one user edits a document, the changes are instantly applied to their local replica (the state of the document ) and asynchronously replicated to the server and any other users who are editing the same document. Leaderless replicationAlso called Dynamo-style: allow any replica to directly accept writes from clients. In some leaderless implementations, the client directly sends its writes to several replicas, while in others, a coordinator node does this on behalf of the client. However, unlike a leader database, that coordinator does not enforce a particular ordering of writes. Version numbers are used to determine which value is newer. quorum reads and writes tries to solve the issue of reading stale data: if there are n replicas, every write must be confirmed by w nodes(synchronized) to be considered successful, and we must query at least r nodes for each read. As long as there is an overlap, that is, w + r &gt; n, we expect to get an up-to-date value when reading. With a smaller w and r you are more likely to read stale values, but this configuration allows lower latency and higher availability. However, even with w + r &gt; n, there are likely to be edge cases where stale values are returned, e.g., sloppy quorum, two writes occur concurrently, a write happens concurrently. Thus, Dynamo-style databases are generally optimized for use cases that can tolerate eventual consistency. How to ensure that eventually all the data is copied to every replica? There are two ways: Read repair: When a client makes a read from several nodes in parallel, it can detect any stale responses. The client then writes the newer value back to the replica that has stale data.This approach works well for values that are frequently read. However, values that are rarely read may be missing from some replicas and thus have reduced durability. Anti-entropy process: a background process that constantly looks for differences in the data between replicas and copies any missing data from one replica to another. Unlike the replication log in leader-based replication, this anti-entropy process does not copy writes in any particular order, and there may be a significant delay before data is copied. Node FailuresIf a follower fails, we can use the Catch-up strategy. On its local disk, each follower keeps a log of the data changes it has received from the leader. If a follower crashes and is restarted, or if the network recovers from a temporary failure, it can connect to the leader and request all the data changes that occurred during the time when the follower was disconnected. When it has applied these changes, it has caught up to the leader and can continue receiving a stream of data changes as before. If a leader fails, we can use the Failover strategy: one of the followers needs to be promoted to be the new leader, clients need to be reconfigured to send their writes to the new leader, and the other followers need to start consuming data changes from the new leader. An automatic failover process usually consists of the following steps: Determining that the leader has failed. There is no foolproof way of detecting if a node is crashed or disconnected from the network, so most systems simply use a timeout: nodes frequently bounce heartbeat message back and forth between each other, and if a node doesn’t respond for some period of time, it is assumed to be dead. Choosing a new leader. This could be done through an election process, or a new leader could be appointed by a previously elected controller node. The best candidate for leadership is usually the replica with the most up-to-date data changes from the old leader. Reconfiguring the system to use the new leader. Clients now need to send their write requests to the new leader. The system needs to ensure that the old leader becomes a follower and recognizes the new leader, if the old leader comes back. Challenges with Replication LagIn an asynchronous architecture, client may see outdated information if the replica has fallen behind. If you stop writing to the database and wait a while, replicas will eventually catch up and become consistent with each other, which is known as eventual consistency. When working with an eventually consistent system, it is worth thinking about how the application behaves if the replication lag increases to several minutes or even hours. If the answer is “no problem,” that’s great. However, if the result is a bad experience for users, it’s important to design the system to provide a stronger guarantee. Reading Your Own WritesWhen new data is submitted, it must be sent to the leader, but when the user views the data, it can be read from a follower. the new data may not yet have reached the replica. To the user, it looks as though the data they submitted was lost read-after-write consistency: This is a guarantee that if the user reloads the page, they will always see any updates they submitted themselves. It makes no promises about other users: other users’ updates may not be visible until some later time. Implementations in leader-based architecture: If only a few things in the application are potentially editable by the user, when reading something that the user may have modified, read it from the leader; otherwise, read it from a follower Use criteria to decide whether to read from the leader, i.e, for one minute after the last update, make all reads from the leader The client can remember the timestamp of its most recent write. If a replica is not sufficiently up to date, query another replica or wait until the replica has caught up. Monotonic ReadsWhen reading from asynchronous followers is that it’s possible for a user to see things moving backward in time because of reading an out-of-date replica. Monotonic reads: if one user makes several reads in sequence, they will not read older data after having previously read newer data. Implementation: each user always makes their reads from the same replica, i.e., the replica can be chosen based on a hash of the user ID. Consistent Prefix Readsconsistent prefix reads: if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order. One implementation is to make sure that any writes that are causally related to each other are written to the same partition. Concurrency Write ConflictsConcurrency Write issues are inherent in multi-leader and leaderless replication approaches: because they allow multiple writes to happen concurrently, conflicts may occur. Conflict avoidanceIf the application can ensure that all writes for a particular record go through the same leader, then conflicts cannot occur, i.e, route user write requests to the same leader based on the hash of its user id. Converging toward a consistent stateApproaches that are dangerously prone to data loss: Give each write a unique ID (e.g., a timestamp, a long random number, a UUID, or a hash of the key and value), pick the write with the highest ID as the winner, and throw away the other writes. If a timestamp is used, this technique is known as last write wins (LWW) Give each replica a unique ID, and let writes that originated at a higher numbered replica always take precedence over writes that originated at a lower numbered replica. There are some situations, such as caching, in which lost writes are perhaps acceptable. If losing data is not acceptable, LWW is a poor choice for conflict resolution. The only safe way of using a database with LWW is to ensure that a key is only written once and thereafter treated as immutable, thus avoiding any concurrent updates to the same key. For example, a recommended way of using Cassandra is to use a UUID as the key, thus giving each write operation a unique key Other approaches are: merge the values together, e.g., order them alphabetically and then concatenate them Record the conflict in an explicit data structure that preserves all information, and write application code that resolves the conflict at some later time (perhaps by prompting the user). Reference [1] Designing Data-Intensive Applications, Chapter 05: Replication","categories":[{"name":"Reading Notes","slug":"Reading-Notes","permalink":"https://blog-pika.github.io/categories/Reading-Notes/"},{"name":"Designing Data-Intensive Applications","slug":"Reading-Notes/Designing-Data-Intensive-Applications","permalink":"https://blog-pika.github.io/categories/Reading-Notes/Designing-Data-Intensive-Applications/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"https://blog-pika.github.io/tags/Data-Structure/"},{"name":"Database","slug":"Database","permalink":"https://blog-pika.github.io/tags/Database/"},{"name":"SSTable","slug":"SSTable","permalink":"https://blog-pika.github.io/tags/SSTable/"}]},{"title":"DDIA - Ch8 - Pitfalls in Distributed System","slug":"Distributed-System-101/DDIA-Ch8-Pitfalls-in-distribuetd-system","date":"2019-08-11T07:00:00.000Z","updated":"2019-08-12T06:01:23.829Z","comments":true,"path":"2019/08/11/Distributed-System-101/DDIA-Ch8-Pitfalls-in-distribuetd-system/","link":"","permalink":"https://blog-pika.github.io/2019/08/11/Distributed-System-101/DDIA-Ch8-Pitfalls-in-distribuetd-system/","excerpt":"AbstractThis blog is my reading notes for Designing Data-Intensive Applications, Chapter 08: The Trouble With Distributed System. Working with distributed systems is fundamentally different from writing software on a single computer. The main difference is that we can know nothing for sure. This chapter first introduces how components can be unreliable in a distributed system, and why designing distributed system can be challenging. Then the author discusses how to get truth from uncertainty. He also introduces some theoretical models and how can a distributed algorithm defined as correct.","text":"AbstractThis blog is my reading notes for Designing Data-Intensive Applications, Chapter 08: The Trouble With Distributed System. Working with distributed systems is fundamentally different from writing software on a single computer. The main difference is that we can know nothing for sure. This chapter first introduces how components can be unreliable in a distributed system, and why designing distributed system can be challenging. Then the author discusses how to get truth from uncertainty. He also introduces some theoretical models and how can a distributed algorithm defined as correct. Typical UnReliable ComponentUnreliable NetworkMost distributed systems are asynchronous networks and have unbounded delays, that is, they try to deliver packets as quickly as possible, but there is no upper limit on the time it may take for a packet to arrive. So Whenever you try to send a packet over the network, it may be lost or arbitrarily delayed. Likewise, the reply may be lost or delayed, so if you don’t get a reply, you have no idea whether the message got through. Though TCP retries transparently, in the application level, you may also retry a few times, wait for a timeout to elapse, and eventually declare the node dead if you don’t hear back within the timeout. Even better, rather than using configured constant timeouts, systems can continually measure response times and their variability, and automatically adjust timeouts according to the observed response time distribution. Akka and Cassandra use a Phi Accrual failure detector to dynamically adjust the timeout. You do need to know how your software reacts to network problems and ensure that the system can recover from them. If your network is normally fairly reliable, a valid approach may be to simply show an error message to users while your network is experiencing problems. Unreliable ClockEach machine on the network has its own clock, which is an actual hardware device: usually a quartz crystal oscillator. The quartz clock in a computer is not very accurate: it drifts (runs faster or slower than it should), so each machine has its own notion of time, which may be slightly faster or slower than on other machines. A commonly used mechanism to synchronize clocks is the Network Time Protocol (NTP), which allows the computer clock to be adjusted according to the time reported by a group of servers. Modern computers have at least two different kinds of clocks: a time-of-day clock to get timestamp, and a monotonic clock to get time duration. Time-of-day clock returns the current date and time according to some calendar. However, it may be forcibly reset by the NTP server, suddenly jump forward or back in time. Even worse, NTP synchronization can only be as good as the network delay, so there is a limit to its accuracy when you’re on a congested network with variable packet delays. Therefore, replying on Time-of-day clock is dangerous. On the contrary, NTP cannot cause the monotonic clock to jump forward or backward. Since a monotonic clock is guaranteed to always move forward, it is usually fine to use a monotonic clock to measure time duration in distributed system. There are two ways to get reliable timestamps Logical Clocks: which are based on incrementing counters rather than an oscillating quartz crystal, are a safer alternative for ordering events. Logical clocks do not measure the time of day or the number of seconds elapsed, only the relative ordering of events. Google Spanner provides a TrueTime API, which returns [earliest, latest]. The interval are the earliest possible and the latest possible timestamp. Therefore, the clock knows that the actual current time is somewhere within that interval. Since Spanner uses atomic clocks, the interval are usually within 7ms. In order to ensure that transaction timestamps reflect causality, Spanner deliberately waits for the length of the confidence interval before committing a read-write transaction. By doing so, it ensures that any transaction that may read the data is at a sufficiently later time, so their confidence intervals do not overlap. A problem of clock is that even if it is defective, most things will seem to work fine. If some piece of software is relying on an accurately synchronized clock, the result is more likely to be silent and subtle data loss than a dramatic crash. Thus, if you use software that requires synchronized clocks, it is essential that you also carefully monitor the clock offsets between all the machines. Any node whose clock drifts too far from the others should be declared dead and removed from the cluster. Process PauseA running thread can be preempted at any point and resume it at some later time, without the thread even noticing: e.g.: Many programming language runtime have a GC that occasionally needs to stop all running threads, which can pauses for several minutes in the worst case! Even so-called “concurrent” garbage collectors cannot fully run in parallel with the application code Execution may also be suspended and resumed arbitrarily, e.g., when VM migrates from one host to another host, when the operating system context-switches to another thread, when the user closes the lid of their laptop If the application performs synchronous disk access, a thread may be paused waiting for a slow disk I/O operation to complete. If the operating system is configured to allow paging, a simple memory access may result in a page fault that requires a page from disk to be loaded into memory. In many languages, disk access can happen surprisingly, even if the code doesn’t explicitly mention file access. For example, the Java class loader lazily loads class files when they are first used, which could happen at any time in the program execution. A Unix process can be paused by sending it the SIGSTOP signal Therefore, we can’t assume anything about timing. A node in a distributed system must assume that its execution can be paused for a significant length of time at any point, even in the middle of a function. During the pause, the rest of the world keeps moving and may even declare the paused node dead because it’s not responding. Eventually, the paused node may continue running, without even noticing that it was asleep until it checks its clock sometime later. In the code below, if code enters if (lease.isValid() but suddenly preempted. During its pause other nodes can become the new leader. If it continues to process request as it is supposed to do as a leader, the system may go wrong. 123456789101112while (true) &#123; request = getIncomingRequest(); // Ensure that the lease always has at least 10 seconds remaining if (lease.expiryTimeMillis - System.currentTimeMillis() &lt; 10000) &#123; lease = lease.renew(); &#125; if (lease.isValid()) &#123; process(request); &#125;&#125; We can limit the impact of garbage collection in the following two ways: Treat GC pauses like brief planned outages of a node, and to let other nodes handle requests from clients while one node is collecting its garbage. If the runtime can warn the application that a node soon requires a GC pause, the application can stop sending new requests to that node, wait for it to finish processing outstanding requests, and then perform the GC while no requests are in progress Use the garbage collector only for short-lived objects and to restart processes periodically, before they accumulate enough long-lived objects to require a full GC of long-lived objects Challenges in Distributed SystemUncertainty is the defining characteristic of distributed systems. Whenever software tries to do anything involving other nodes, there is the possibility that it may occasionally fail, or randomly go slow, or not respond at all (and eventually time out). In distributed systems, we try to build tolerance of partial failures into software, so that the system as a whole may continue functioning even when some of its constituent parts are broken. To tolerate faults, the first step is to detect them, but even that is hard. Most systems don’t have an accurate mechanism of detecting whether a node has failed, so most distributed algorithms rely on timeouts to determine whether a remote node is still available. However, timeouts can’t distinguish between network and node failures, and variable network delay sometimes causes a node to be falsely suspected of crashing. Once a fault is detected, making a system tolerate it is not easy either: there is no global variable, no shared memory, no common knowledge or any other kind of shared state between the machines. Nodes can’t even agree on what time it is, let alone on anything more profound. The only way information can flow from one node to another is by sending it over the unreliable network. Major decisions cannot be safely made by a single node, so we require protocols that enlist help from other nodes and try to get a quorum to agree. If you’re used to writing software in the idealized mathematical perfection of a single computer, where the same operation always deterministically returns the same result, then moving to the messy physical reality of distributed systems can be a bit of a shock. Conversely, distributed systems engineers will often regard a problem as trivial if it can be solved on a single computer, and indeed a single computer can do a lot nowadays. If you can avoid opening Pandora’s box and simply keep things on a single machine, it is generally worth doing so. However, scalability is not the only reason for wanting to use a distributed system. Fault tolerance and low latency (by placing data geographically close to users) are equally important goals, and those things cannot be achieved with a single node. How to know the TruthA node cannot necessarily trust its own judgment of a situation. A distributed system cannot exclusively rely on a single node, because a node may fail at any time, potentially leaving the system stuck and unable to recover. Instead, many distributed algorithms rely on a quorum, that is, voting among the nodes: decisions require some minimum number of votes from several nodes in order to reduce the dependence on any one particular node. Most commonly, the quorum is an absolute majority of more than half the nodes. System Model and Algorithms ScopeAlgorithms for distributed system need to be written in a way that does not depend too heavily on the details of the hardware and software configuration on which they are run. This in turn requires that we somehow formalize the kinds of faults that we expect to happen in a system. We do this by defining a system model, which is an abstraction that describes what things an algorithm may assume. With regard to timing assumptions, three system models are in common use: Synchronous model: The synchronous model assumes bounded network delay, bounded process pauses, and bounded clock error. Bounded does not imply exactly zero; it just means that they will never exceed some fixed upper bound. The synchronous model is not a realistic model of most practical systems Partially synchronous model: Partial synchrony means that a system behaves like a synchronous system most of the time, but it sometimes exceeds the bounds for network delay, process pauses, and clock drift. This is a realistic model of many systems. Asynchronous model: In this model, an algorithm is not allowed to make any timing assumptions. In fact, it does not even have a clock, so it cannot use timeouts. Some algorithms can be designed for the asynchronous model, but it is very restrictive. With regard to node failures. The three most common system models are: Crash-stop faults: a node can fail in only by crashing: suddenly stop responding at any moment, and thereafter that node is gone foreverCrash-recovery faults: nodes may crash at any moment, and perhaps start responding again after some unknown time. Stable storage is assumed to be preserved across crashes, while the in-memory state is assumed to be lostByzantine faults: Nodes may do absolutely anything, including trying to trick and deceive other nodes. For modeling real systems, the partially synchronous model with crash-recovery faults is generally the most useful model. But how do distributed algorithms cope with that model? To define what it means for an algorithm to be correct, we need to define safety and liveness properties. Safety is often informally defined as nothing bad happens, and liveness as something good eventually happens.: Safety: If a safety property is violated, we can point at a particular point in time at which it was broken (for example, if the uniqueness property was violated, we can identify the particular operation in which a duplicate fencing token was returned). After a safety property has been violated, the violation cannot be undone—the damage is already done. Liveness: may not hold at some point in time (for example, a node may have sent a request but not yet received a response), but there is always hope that it may be satisfied in the future (namely by receiving a response). For distributed algorithms, it is common to require that safety properties always hold, in all possible situations of a system model. That is, even if all nodes crash, or the entire network fails, the algorithm must nevertheless ensure that it does not return a wrong result. However, with liveness properties we are allowed to make caveats: for example, we could say that a request needs to receive a response only if a majority of nodes have not crashed, and only if the network eventually recovers from an outage. The definition of the partially synchronous model requires that eventually the system returns to a synchronous state—that is, any period of network interruption lasts only for a finite duration and is then repaired. Reference [1] Designing Data-Intensive Applications, Chapter 08: The Trouble With Distributed System","categories":[{"name":"Reading Notes","slug":"Reading-Notes","permalink":"https://blog-pika.github.io/categories/Reading-Notes/"},{"name":"Designing Data-Intensive Applications","slug":"Reading-Notes/Designing-Data-Intensive-Applications","permalink":"https://blog-pika.github.io/categories/Reading-Notes/Designing-Data-Intensive-Applications/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"https://blog-pika.github.io/tags/Distributed-System/"},{"name":"DDIA","slug":"DDIA","permalink":"https://blog-pika.github.io/tags/DDIA/"}]},{"title":"DDIA - Ch6 - Partition","slug":"Distributed-System-101/DDIA-Ch6-Partition","date":"2019-08-05T07:00:00.000Z","updated":"2019-08-09T03:01:42.849Z","comments":true,"path":"2019/08/05/Distributed-System-101/DDIA-Ch6-Partition/","link":"","permalink":"https://blog-pika.github.io/2019/08/05/Distributed-System-101/DDIA-Ch6-Partition/","excerpt":"AbstractThis blog is my reading notes for Designing Data-Intensive Applications, Chapter 06: Partition. Partitioning means splitting a large dataset into smaller subsets, which are then placed on different nodes in a shared-nothing cluster. It is necessary when you have so much data that storing and processing it on a single machine is no longer feasible. Also it has advantages over scalability: a large dataset can be distributed across many disks, and the query load can be distributed across many processors.","text":"AbstractThis blog is my reading notes for Designing Data-Intensive Applications, Chapter 06: Partition. Partitioning means splitting a large dataset into smaller subsets, which are then placed on different nodes in a shared-nothing cluster. It is necessary when you have so much data that storing and processing it on a single machine is no longer feasible. Also it has advantages over scalability: a large dataset can be distributed across many disks, and the query load can be distributed across many processors. In this blog we will look at the following topics one by one: How to partition datasets and avoid hot spot? How to rebalance Data? How to use secondary indexes in partition? How to route request? Approaches to PartitioningWe have following ways of partition: Key Range Partition: assign a continuous range of keys to each partition. Within each partition, keys are kept in sorted order. The advantage is that efficient range queries are possible, but there is a risk of hot spots if the application often accesses keys that are close together in the sorted order. Partitions are typically rebalanced dynamically by splitting the range into two sub-ranges when a partition gets too big. Hash Partition: a hash function is applied to each key, and a partition owns a range of hashes. Range queries are inefficient, but data are distributed more evenly. When partitioning by hash, it is common to create a fixed number of partitions in advance, to assign several partitions to each node, and to move partitions from one node to another when nodes are added or removed. Dynamic partitioning can also be used. Hybrid Partition: a table can be declared with a compound primary key consisting of several columns. Only the first part of that key is hashed to determine the partition, but the other columns are used as a concatenated index for sorting the data. This approach enables an elegant data model for one-to-many relationships. For example, on a social media site, one user may post many updates. If the primary key for updates is chosen to be (user_id, update_timestamp), then you can efficiently retrieve all updates made by a particular user within some time interval, sorted by timestamp. If the partitioning is unfair, so that some partitions have more data or queries than others, we call it skewed. A partition with disproportionately high load is called a hot spot. Today, most data systems are not able to automatically compensate for such a highly skewed workload, so it’s the responsibility of the application to reduce the skew. One simple technique is that, if one key is known to be very hot, add a random number to the beginning or end of the key can split the writes to the key evenly across different keys, i.e., distributed to different partitions. The disadvantage is that this technique now requires additional bookkeeping and reading the data now need to query from all keys and combine them. RebalancingRebalancing is the process of moving load from one node in the cluster to another, which is called for the following changes: query throughput increases, dataset size increases, node fails. Usually it is expected to meet some minimum requirements: After rebalancing, the load should be shared fairly between the nodes in the cluster. While rebalancing is happening, the database should continue accepting reads and writes. No more data than necessary should be moved between nodes, to make rebalancing fast and to minimize the network and disk I/O load. We also have 3 approaches to do that: Fixed number of partitions: can be used if the dataset is not highly variable: create many more partitions than there are nodes, and assign several partitions to each node. If a node is added to the cluster, the new node can steal a few partitions from every existing node until partitions are fairly distributed once again. In this partition, the number of partitions is usually fixed when the database is first set up and not changed afterward. Also, partition splitting is usually not implemented. The number of partitions configured at the outset = the maximum number of nodes * number of partitions per node, so you need to choose it high enough to accommodate future growth. Used in Riak, Elasticsearch, Couchbase, and Voldemort Dynamic partitioning proportional to the size of the dataset: When a partition grows to exceed a configured size, it is split into two partitions so that approximately half of the data ends up on each side of the split. Conversely, if lots of data is deleted and a partition shrinks below some threshold, it can be merged with an adjacent partition. In the initial state when the dataset is small or empty, an initial set of partitions to be configured. Used by HBase and MongoDB Dynamic partitioning proportional to the number of nodes: When a new node joins the cluster, it randomly chooses a fixed number of existing partitions to split, and then takes ownership of one half of each of those split partitions while leaving the other half of each partition in place. The randomization can produce unfair splits, but when averaged over a larger number of partitions, the new node ends up taking a fair share of the load from the existing nodes. Used in Cassandra and Ketama. Fully automated rebalancing can be convenient but unpredictable. Rebalancing is an expensive operation, if it is not done carefully, this process can overload the network or the nodes and harm the performance of other requests while the rebalancing is in progress. In combination with automatic failure detection, other nodes can conclude an overloaded node to be dead and automatically rebalance the cluster to move load away from it, which may cause a cascading failure. Secondary indexsince dataset is partitioned by primary key, secondary indexes don’t map neatly to partitions. Thus, many key-value stores have avoided secondary indexes because of their added implementation complexity, but some have started adding them because they are so useful for data modeling. There are two methods: Document-partitioned indexes: each partition is completely separate and maintains its own local secondary indexes, covering only the documents in that partition. This means that only a single partition needs to be updated on write, but a read of the secondary index requires a scatter/gather across all partitions, which is expensive. Term-partitioned indexes: construct a global secondary index that covers data in all partitions. The global index is also partitioned to avoid bottleneck, but it can be partitioned differently from the primary key index. When a document is written, a distributed transaction across all partitions that need to be updated is required, which is expensive. However, a read can be served from a single partition and is efficient. Request RoutingWhen a client wants to make a request, how does it know which node to connect to? On a high level, there are a few different approaches to this problem. Allow clients to contact any node. If that node does not own the partition, it forwards the request to the appropriate node Send all requests from clients to a routing tier first, which determines the node that should handle each request and forwards it accordingly. This routing tier does not itself handle any requests; it only acts as a partition-aware load balancer. Require that clients be aware of the partitioning and the assignment of partitions to nodes, thus they can connect directly to the appropriate node without any intermediary. In all cases, the key problem is: how does the component making the routing decision and learn about changes in the assignment of partitions to nodes? This is an instance of a more general problem called service discovery Any piece of software that is accessible over a network has this problem, especially if it is aiming for high availability. Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata. Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. Other actors, such as the routing tier or the partitioning-aware client, can subscribe to this information in ZooKeeper. Whenever a partition changes ownership, or a node is added or removed, ZooKeeper notifies the routing tier so that it can keep its routing information up to date. Or the system can use a gossip protocol among the nodes to disseminate any changes in cluster state. Requests can be sent to any node, and that node forwards them to the appropriate node for the requested partition. Cassandra and Riak adopt this way. Reference [1] Designing Data-Intensive Applications, Chapter 06: Partition","categories":[{"name":"Reading Notes","slug":"Reading-Notes","permalink":"https://blog-pika.github.io/categories/Reading-Notes/"},{"name":"Designing Data-Intensive Applications","slug":"Reading-Notes/Designing-Data-Intensive-Applications","permalink":"https://blog-pika.github.io/categories/Reading-Notes/Designing-Data-Intensive-Applications/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"https://blog-pika.github.io/tags/Distributed-System/"},{"name":"DDIA","slug":"DDIA","permalink":"https://blog-pika.github.io/tags/DDIA/"},{"name":"Partition","slug":"Partition","permalink":"https://blog-pika.github.io/tags/Partition/"}]},{"title":"DDIA - Ch5 - Replication","slug":"Distributed-System-101/DDIA-Ch5-Replicattion","date":"2019-08-01T07:00:00.000Z","updated":"2019-08-09T03:04:04.430Z","comments":true,"path":"2019/08/01/Distributed-System-101/DDIA-Ch5-Replicattion/","link":"","permalink":"https://blog-pika.github.io/2019/08/01/Distributed-System-101/DDIA-Ch5-Replicattion/","excerpt":"AbstractThis blog is my reading notes for Designing Data-Intensive Applications, Chapter 05: Replication. Replication means keeping a copy of the same data on multiple machines that are connected via a network. It has following benefits: High availability: Keeping the system running, even when one/several machines goes down Latency: Placing data geographically close to users, so that users can interact with it faster Scalability: Being able to handle a higher volume of reads than a single machine could handle, by performing reads on replicas","text":"AbstractThis blog is my reading notes for Designing Data-Intensive Applications, Chapter 05: Replication. Replication means keeping a copy of the same data on multiple machines that are connected via a network. It has following benefits: High availability: Keeping the system running, even when one/several machines goes down Latency: Placing data geographically close to users, so that users can interact with it faster Scalability: Being able to handle a higher volume of reads than a single machine could handle, by performing reads on replicas However, keeping data consistent between replicas are not as simple as it looks. We need to consider the following issues: node failures; unreliable networks; and tradeoffs around replica consistency, durability, availability, and latency. Let’s look at them one by one: should we use synchronous or asynchronous? which granularity of Replication Logs should we use? which kind of replication architecture: Single-leader, multiple-leader, or leaderless? should we provide consistency stronger than eventual consistency? How to handle node failure and unreliable network? How to handle concurrency write conflicts? Synchronous vs Asynchronous synchronous: the leader waits until follower has confirmed that it received the write before reporting success to the user, and before making the write visible to other clients. asynchronous: the leader sends the message, but doesn’t wait for a response from the follower. semi-synchronous: one of the followers is synchronous, and the others are asynchronous. If the synchronous follower becomes unavailable or slow, one of the asynchronous followers is made synchronous. In practice, if you enable synchronous replication on a database, it usually means semi-synchronous. Synchronous is impractical because the leader must block all writes and wait until the synchronous replica is available again. Often, leader-based replication is configured to be completely asynchronous. In this case, if the leader fails and is not recoverable, any writes that have not yet been replicated to followers are lost. This means that a write is not guaranteed to be durable, even if it has been confirmed to the client. However, a fully asynchronous configuration has the advantage that the leader can continue processing writes, even if all of its followers have fallen behind. Granularity of Replication Logs Statement-based replication: Deprecated. In the simplest case, the leader logs every write request (statement) that it executes and sends that statement log to its followers. For a relational database, this means that every INSERT, UPDATE, or DELETE statement is forwarded to followers, and each follower parses and executes that SQL statement as if it had been received from a client. Statements may be nondeterministic or have side effects, which causes replications diverge. Write-ahead log (WAL) shipping: the log is an append-only sequence of bytes containing all writes to the database. We can use the exact same log to build a replica on another node. The disadvantage is that it makes replication closely coupled to the storage engine. A newer software version usually had format compatibility issue, which requires downtime for upgrading. Logical (row-based) log replication: A logical log for a relational database is usually a sequence of records describing writes to database tables at the granularity of a row. Replication architecturesThere are three main approaches to replication and each approach has advantages and disadvantages. Single-leader replication is popular because it is fairly easy to understand and there is no conflict resolution to worry about. Multi-leader and leaderless replication can be more robust in the presence of faulty nodes, network interruptions, and latency spikes—at the cost of being harder to reason about and providing only very weak consistency guarantees. Single-leader replication One of the replicas is designated the leader. Write requests of clients can only go to leader, which first writes the new data to its local storage. The other replicas are known as followers . Whenever the leader writes new data to its local storage, it also sends the data change to all of its followers. Each follower takes the log from the leader and updates its local copy of the database accordingly. When a client wants to read from the database, it can query either the leader or any of the followers. However, writes are only accepted on the leader. Multi-leader replicationClients send each write to one of several leader nodes, any of which can accept writes. The leaders send streams of data change events to each other and to any follower nodes. Multi-leader replication has advantage on performance, tolerance of datacenter outages, tolerance of network problems.However, it rarely makes sense to use a multi-leader setup within a single datacenter, because the benefits rarely outweigh the added complexity: the same data may be concurrently modified in two different datacenters, and those write conflicts must be resolved. Here are some common use cases for multi-leader replication: Multi-datacenter operation: Within each datacenter, regular leader-follower replication is used; between datacenters, each datacenter’s leader replicates its changes to the leaders in other datacenters. Clients with offline operation: you have an application that needs to continue to work with multiple devices while they are disconnected from the internet.In this case, every device has a local database that acts as a leader (it accepts write requests), and there is an asynchronous multi-leader replication process (sync) between the replicas on all of your devices. Collaborative editing: When one user edits a document, the changes are instantly applied to their local replica (the state of the document ) and asynchronously replicated to the server and any other users who are editing the same document. Leaderless replicationAlso called Dynamo-style: allow any replica to directly accept writes from clients. In some leaderless implementations, the client directly sends its writes to several replicas, while in others, a coordinator node does this on behalf of the client. However, unlike a leader database, that coordinator does not enforce a particular ordering of writes. Version numbers are used to determine which value is newer. quorum reads and writes tries to solve the issue of reading stale data: if there are n replicas, every write must be confirmed by w nodes(synchronized) to be considered successful, and we must query at least r nodes for each read. As long as there is an overlap, that is, w + r &gt; n, we expect to get an up-to-date value when reading. With a smaller w and r you are more likely to read stale values, but this configuration allows lower latency and higher availability. However, even with w + r &gt; n, there are likely to be edge cases where stale values are returned, e.g., sloppy quorum, two writes occur concurrently, a write happens concurrently. Thus, Dynamo-style databases are generally optimized for use cases that can tolerate eventual consistency. How to ensure that eventually all the data is copied to every replica? There are two ways: Read repair: When a client makes a read from several nodes in parallel, it can detect any stale responses. The client then writes the newer value back to the replica that has stale data.This approach works well for values that are frequently read. However, values that are rarely read may be missing from some replicas and thus have reduced durability. Anti-entropy process: a background process that constantly looks for differences in the data between replicas and copies any missing data from one replica to another. Unlike the replication log in leader-based replication, this anti-entropy process does not copy writes in any particular order, and there may be a significant delay before data is copied. Node FailuresIf a follower fails, we can use the Catch-up strategy. On its local disk, each follower keeps a log of the data changes it has received from the leader. If a follower crashes and is restarted, or if the network recovers from a temporary failure, it can connect to the leader and request all the data changes that occurred during the time when the follower was disconnected. When it has applied these changes, it has caught up to the leader and can continue receiving a stream of data changes as before. If a leader fails, we can use the Failover strategy: one of the followers needs to be promoted to be the new leader, clients need to be reconfigured to send their writes to the new leader, and the other followers need to start consuming data changes from the new leader. An automatic failover process usually consists of the following steps: Determining that the leader has failed. There is no foolproof way of detecting if a node is crashed or disconnected from the network, so most systems simply use a timeout: nodes frequently bounce heartbeat message back and forth between each other, and if a node doesn’t respond for some period of time, it is assumed to be dead. Choosing a new leader. This could be done through an election process, or a new leader could be appointed by a previously elected controller node. The best candidate for leadership is usually the replica with the most up-to-date data changes from the old leader. Reconfiguring the system to use the new leader. Clients now need to send their write requests to the new leader. The system needs to ensure that the old leader becomes a follower and recognizes the new leader, if the old leader comes back. Challenges with Replication LagIn an asynchronous architecture, client may see outdated information if the replica has fallen behind. If you stop writing to the database and wait a while, replicas will eventually catch up and become consistent with each other, which is known as eventual consistency. When working with an eventually consistent system, it is worth thinking about how the application behaves if the replication lag increases to several minutes or even hours. If the answer is “no problem,” that’s great. However, if the result is a bad experience for users, it’s important to design the system to provide a stronger guarantee. Reading Your Own WritesWhen new data is submitted, it must be sent to the leader, but when the user views the data, it can be read from a follower. the new data may not yet have reached the replica. To the user, it looks as though the data they submitted was lost read-after-write consistency: This is a guarantee that if the user reloads the page, they will always see any updates they submitted themselves. It makes no promises about other users: other users’ updates may not be visible until some later time. Implementations in leader-based architecture: If only a few things in the application are potentially editable by the user, when reading something that the user may have modified, read it from the leader; otherwise, read it from a follower Use criteria to decide whether to read from the leader, i.e, for one minute after the last update, make all reads from the leader The client can remember the timestamp of its most recent write. If a replica is not sufficiently up to date, query another replica or wait until the replica has caught up. Monotonic ReadsWhen reading from asynchronous followers is that it’s possible for a user to see things moving backward in time because of reading an out-of-date replica. Monotonic reads: if one user makes several reads in sequence, they will not read older data after having previously read newer data. Implementation: each user always makes their reads from the same replica, i.e., the replica can be chosen based on a hash of the user ID. Consistent Prefix Readsconsistent prefix reads: if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order. One implementation is to make sure that any writes that are causally related to each other are written to the same partition. Concurrency Write ConflictsConcurrency Write issues are inherent in multi-leader and leaderless replication approaches: because they allow multiple writes to happen concurrently, conflicts may occur. Conflict avoidanceIf the application can ensure that all writes for a particular record go through the same leader, then conflicts cannot occur, i.e, route user write requests to the same leader based on the hash of its user id. Converging toward a consistent stateApproaches that are dangerously prone to data loss: Give each write a unique ID (e.g., a timestamp, a long random number, a UUID, or a hash of the key and value), pick the write with the highest ID as the winner, and throw away the other writes. If a timestamp is used, this technique is known as last write wins (LWW) Give each replica a unique ID, and let writes that originated at a higher numbered replica always take precedence over writes that originated at a lower numbered replica. There are some situations, such as caching, in which lost writes are perhaps acceptable. If losing data is not acceptable, LWW is a poor choice for conflict resolution. The only safe way of using a database with LWW is to ensure that a key is only written once and thereafter treated as immutable, thus avoiding any concurrent updates to the same key. For example, a recommended way of using Cassandra is to use a UUID as the key, thus giving each write operation a unique key Other approaches are: merge the values together, e.g., order them alphabetically and then concatenate them Record the conflict in an explicit data structure that preserves all information, and write application code that resolves the conflict at some later time (perhaps by prompting the user). Reference [1] Designing Data-Intensive Applications, Chapter 05: Replication","categories":[{"name":"Reading Notes","slug":"Reading-Notes","permalink":"https://blog-pika.github.io/categories/Reading-Notes/"},{"name":"Designing Data-Intensive Applications","slug":"Reading-Notes/Designing-Data-Intensive-Applications","permalink":"https://blog-pika.github.io/categories/Reading-Notes/Designing-Data-Intensive-Applications/"}],"tags":[{"name":"Replication","slug":"Replication","permalink":"https://blog-pika.github.io/tags/Replication/"},{"name":"Distributed System","slug":"Distributed-System","permalink":"https://blog-pika.github.io/tags/Distributed-System/"},{"name":"DDIA","slug":"DDIA","permalink":"https://blog-pika.github.io/tags/DDIA/"}]}]}