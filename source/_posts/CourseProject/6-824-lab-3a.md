---
title: 6.824 Lab 3A - Key/value service without log compaction
tags: [6.824]
categories: [Project]
---

This blog is my notes for [MIT 6.824 Lab 3A: Key/value service without log compaction](https://pdos.csail.mit.edu/6.824/labs/lab-kvraft.html).

<!-- more -->

## Design Overview

The overview is as follow:

![Design Overview](/images/CourseProject/6.824_Lab3A_Overview.jpg)

The steps are as follow:

1. The Client uses its `Clerk` to send GET/PUT/APPEND RPC to leader server. If a follower server receives client request, it should reply with `isLeader = false`, then the client can try another server until it found the leader server. If the request is not successful, e.g., due to server crashes, partition, leadership lost, network issues, the client should retry until receives a successful reply.
2. When the leader KV-service receives RPC args from client, it checks if the request is duplicated. If yes, it replies with the data in the in-memory DB.
3. If the request is not duplicated, leader KV-service submits this log to Raft-Service and is blocked a golang channel. It will be waken up once notified by `ApplyDaemon`.
4. Leader Raft-Service duplicates logs to other followers and once the consensus is reached, it sends committed log to the `ApplyDaemon` via a golang channel.
5. Once `ApplyDaemon` receives logs from Raft-Service, it applies the logs to in-memory database. (All the alive servers should do this).
6. The leader KV-service replies RPC with corresponding information.

Since the client need to retry the request, we should use `clientId + requestID` to distinguish every different request. If the server finds out that it loses its leadership, by checking `isLeader` and `term` returned by Raft, it should stop execution and replies with `isLeader = false`.

Client only sends new request when the old request is acknowledged. Therefore, every request except the current one are committed and durable in Raft. Raft takes care of consensus, crash and partition.

## FAQ

> Q: Are KV-Service and Raft-Server paired? In other words, a server in this Lab has both a KV-service and a Raft-Service?

A: Yes

> Q: How to handle concurrent requests from different clients?

A: All the requests are submitted to the Raft leader, which has a single thread-safe entry: `kv.rf.Start(args.getCommand())`. Therefore, all the concurrent requests will be serialized inside Raft leader. Also, this sequence will be maintained inside KV-server afterwards for processing.

> Q: How to guarantee that every request is executed only once.

If every request is executed at-least-once and at-most-once, then it is executed exactly-once.

* at-least-once: If the client does not receive the ack of a request from KV-server, it will resend the request infinitely until success. Also, as long as the request is committed, Raft guarantees logs' durability in spite of servers crash or leadership change.
* at-most-once: Every Request comes with a `ClientId` and a `RequestId`. We can check if a request has been executed by checking `ClientId` + `RequestId`. Therefore, every request will only be executed at-most-once.

> Q: Will this KV-Server has split-brain issue? That is, is it possible that we receive stale data from old leader?

A: No. A Raft command need to be committed before applied. A old leader cannot commit the log because:

 * if it is in a minority network, it cannot commit the log
 * if it is in majority network, it will find itself out-of-date

> Q: Can KV-Server accepts the `appliedLog` from the follower Raft-Server? This question is identical to do we need to check if the server is still leader when we receive log from Raft.  

A: No, the followers may lag behind. If the leader has accepted some write requests and acks the client, a follower's return data may be stale, which violates `strong consistency`. Some other systems may relax the consistency model for better performance, e.g., Zookeeper can read from the follower.

> Q: In KV-server, for every client, do we need to memorize all the logs (or at least the latest log) we have applied?

A: No. Remember the requestId of the latest applied log is good enough. If the request is stale, i.e., `args.requestId <= appliedId[args.clientId]`, we don't need to submit it to the raft server. If it is a write request, ignore it; if it is a read request, get the value from KV-Server in-memory key-value pair.

> Q: When applyDaemon goroutine receives log from Raft, it should notify the goroutine of KV service. What can we use to identify a log?

A: `ClientId` + `RequestId` may do the trick. But using the index returned by Raft should lead to more concise and less error-prone code. 

> Q: If we use the index returned by `kv.rf.Start(log)`, we may face the following race condition:
![Race Condition](/images/CourseProject/6.824_Lab3A_Race_Condition.jpg)

A: KV server will wait and timeouts, so client will issue a new request, so this race condition will not affect correctness. Also, the time Raft need to reach consensus is relatively long (compared to single node execution), normally this race condition won't happen.

> Q: In the test file, why do we need both `end` and `endname`?

A: This is similar to file and file name. The server has `end` and the config manages `end` by `endname`.

> Q: In the `config.go` file, why is the `endnames` a two-dimension array?

A: `Endname` is unidirectional. We need to record every unidirectional socket for server_i -> server_j, therefore a two-dimension array is needed.

> Q: In the `config.go` file, what does `to` stands for in functions such as `func (cfg *config) connectUnlocked(i int, to []int)`?

A: This is not a good name. `To` stands for a partition of servers.
